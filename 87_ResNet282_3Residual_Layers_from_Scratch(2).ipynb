{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeU25sDk4fcE",
        "outputId": "e82e4703-e3fd-484b-c95b-819da210a396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resnet282\n",
            "Total number of params 4445594\n",
            "Total layers 282\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet44', 'resnet110', 'resnet282']\n",
        "\n",
        "\n",
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    \n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "#Creating the Basic Block with 2 convolutions and 3 x 3 kernel size\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "    #Forward run\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "#Creating our base ResNet model of 3 Residual/Convolution Layers with 16, 32 and 63 layer input sizes\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "#The layer maker function\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def resnet44():\n",
        "    return ResNet(BasicBlock, [7, 7, 7])\n",
        "\n",
        "def resnet110():\n",
        "    return ResNet(BasicBlock, [18, 18, 18])\n",
        "\n",
        "#This will be our ResNet282 model having 3 Residual blocks with 90 - 100 - 90 layers \n",
        "def resnet282():\n",
        "    return ResNet(BasicBlock, [45, 50, 45])\n",
        "\n",
        "def resnet\n",
        "\n",
        "#Print model name with number of parameters \n",
        "def test(net):\n",
        "    import numpy as np\n",
        "    total_params = 0\n",
        "\n",
        "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
        "        total_params += np.prod(x.data.numpy().shape)\n",
        "    print(\"Total number of params\", total_params)\n",
        "    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for net_name in __all__:\n",
        "        if net_name.startswith('resnet'):\n",
        "            print(net_name)\n",
        "            test(globals()[net_name]())\n",
        "            print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the number of parameters\n",
        "model  = resnet282()\n",
        "\n",
        "#model  = resnet44()\n",
        "#model  = resnet110()\n",
        "\n",
        "def count_parameters(model):\n",
        "   return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        " \n",
        "print(count_parameters(model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXGjUOFK40hI",
        "outputId": "1506d548-c6de-43e1-c18f-8aa2a196515a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4445594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse"
      ],
      "metadata": {
        "id": "MbBidOzj5kNJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate help and usage messages\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
        "parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
        "parser.add_argument('--resume', '-r', action='store_true',\n",
        "                    help='resume from checkpoint')\n",
        "parser.add_argument('-f')\n",
        "args = parser.parse_args()\n"
      ],
      "metadata": {
        "id": "-7IFaNqv5l_Y"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#use cuda if available to accelerate the computation\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
      ],
      "metadata": {
        "id": "ImLqUGwR5m87"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the data and preparing the training and test Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "#Define the classes in the data\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uitJtptV5ouE",
        "outputId": "14a2e1f9-12a6-41f8-e21f-2e266ab790d4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('==> Building model..')\n",
        "\n",
        "#Implement data parallelism if cuda is available\n",
        "\n",
        "#net  = resnet44()\n",
        "#net  = resnet110()\n",
        "net = resnet282()\n",
        "\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IysNaj2E5q44",
        "outputId": "7b524348-2a07-49f0-e459-00106246dd65"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Building model..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if args.resume:\n",
        "    # Load checkpoint.\n",
        "    print('==> Resuming from checkpoint..')\n",
        "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
        "    checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
        "    net.load_state_dict(checkpoint['net'])\n",
        "    best_acc = checkpoint['acc']\n",
        "    start_epoch = checkpoint['epoch']"
      ],
      "metadata": {
        "id": "wdJ9XLv45t6z"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define optimizer, learning rate, decay and loss type\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=1e-1,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
      ],
      "metadata": {
        "id": "mYSYBH1g6gD7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))"
      ],
      "metadata": {
        "id": "115d9l816j4t"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "            \n",
        "        # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ckpt.pth')\n",
        "        best_acc = acc"
      ],
      "metadata": {
        "id": "mMIhnVgA6lGd"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and testing on the data for the given epochs\n",
        "for epoch in range(start_epoch, start_epoch+100):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37QEql6q6mhu",
        "outputId": "5515714a-89e8-4765-f80c-054d88c8c8f2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            "390 391 Loss: 2.122 | Acc: 24.828% (12414/50000)\n",
            "99 100 Loss: 2.027 | Acc: 30.750% (3075/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 1\n",
            "390 391 Loss: 1.478 | Acc: 45.370% (22685/50000)\n",
            "99 100 Loss: 1.531 | Acc: 46.020% (4602/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 2\n",
            "390 391 Loss: 1.122 | Acc: 59.562% (29781/50000)\n",
            "99 100 Loss: 1.300 | Acc: 57.240% (5724/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 3\n",
            "390 391 Loss: 0.916 | Acc: 67.484% (33742/50000)\n",
            "99 100 Loss: 1.576 | Acc: 54.290% (5429/10000)\n",
            "\n",
            "Epoch: 4\n",
            "390 391 Loss: 0.800 | Acc: 72.308% (36154/50000)\n",
            "99 100 Loss: 0.899 | Acc: 69.510% (6951/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 5\n",
            "390 391 Loss: 0.730 | Acc: 74.604% (37302/50000)\n",
            "99 100 Loss: 1.104 | Acc: 65.740% (6574/10000)\n",
            "\n",
            "Epoch: 6\n",
            "390 391 Loss: 0.684 | Acc: 76.450% (38225/50000)\n",
            "99 100 Loss: 0.895 | Acc: 70.620% (7062/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 7\n",
            "390 391 Loss: 0.646 | Acc: 77.814% (38907/50000)\n",
            "99 100 Loss: 1.075 | Acc: 68.770% (6877/10000)\n",
            "\n",
            "Epoch: 8\n",
            "390 391 Loss: 0.612 | Acc: 78.940% (39470/50000)\n",
            "99 100 Loss: 0.723 | Acc: 75.010% (7501/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 9\n",
            "390 391 Loss: 0.600 | Acc: 79.338% (39669/50000)\n",
            "99 100 Loss: 0.802 | Acc: 73.030% (7303/10000)\n",
            "\n",
            "Epoch: 10\n",
            "390 391 Loss: 0.587 | Acc: 79.734% (39867/50000)\n",
            "99 100 Loss: 0.838 | Acc: 72.990% (7299/10000)\n",
            "\n",
            "Epoch: 11\n",
            "390 391 Loss: 0.562 | Acc: 80.744% (40372/50000)\n",
            "99 100 Loss: 0.750 | Acc: 74.510% (7451/10000)\n",
            "\n",
            "Epoch: 12\n",
            "390 391 Loss: 0.551 | Acc: 81.046% (40523/50000)\n",
            "99 100 Loss: 0.679 | Acc: 76.960% (7696/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 13\n",
            "390 391 Loss: 0.542 | Acc: 81.532% (40766/50000)\n",
            "99 100 Loss: 0.793 | Acc: 74.220% (7422/10000)\n",
            "\n",
            "Epoch: 14\n",
            "390 391 Loss: 0.529 | Acc: 81.734% (40867/50000)\n",
            "99 100 Loss: 0.832 | Acc: 73.190% (7319/10000)\n",
            "\n",
            "Epoch: 15\n",
            "390 391 Loss: 0.523 | Acc: 82.110% (41055/50000)\n",
            "99 100 Loss: 0.681 | Acc: 77.620% (7762/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 16\n",
            "390 391 Loss: 0.510 | Acc: 82.556% (41278/50000)\n",
            "99 100 Loss: 0.900 | Acc: 72.120% (7212/10000)\n",
            "\n",
            "Epoch: 17\n",
            "390 391 Loss: 0.503 | Acc: 82.746% (41373/50000)\n",
            "99 100 Loss: 0.707 | Acc: 76.840% (7684/10000)\n",
            "\n",
            "Epoch: 18\n",
            "390 391 Loss: 0.502 | Acc: 82.854% (41427/50000)\n",
            "99 100 Loss: 0.861 | Acc: 72.550% (7255/10000)\n",
            "\n",
            "Epoch: 19\n",
            "390 391 Loss: 0.495 | Acc: 83.022% (41511/50000)\n",
            "99 100 Loss: 0.817 | Acc: 73.940% (7394/10000)\n",
            "\n",
            "Epoch: 20\n",
            "390 391 Loss: 0.488 | Acc: 83.376% (41688/50000)\n",
            "99 100 Loss: 0.907 | Acc: 70.350% (7035/10000)\n",
            "\n",
            "Epoch: 21\n",
            "390 391 Loss: 0.477 | Acc: 83.580% (41790/50000)\n",
            "99 100 Loss: 0.635 | Acc: 79.000% (7900/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 22\n",
            "390 391 Loss: 0.473 | Acc: 83.692% (41846/50000)\n",
            "99 100 Loss: 0.659 | Acc: 78.180% (7818/10000)\n",
            "\n",
            "Epoch: 23\n",
            "390 391 Loss: 0.468 | Acc: 83.986% (41993/50000)\n",
            "99 100 Loss: 1.109 | Acc: 66.800% (6680/10000)\n",
            "\n",
            "Epoch: 24\n",
            "390 391 Loss: 0.467 | Acc: 84.002% (42001/50000)\n",
            "99 100 Loss: 0.718 | Acc: 76.190% (7619/10000)\n",
            "\n",
            "Epoch: 25\n",
            "390 391 Loss: 0.465 | Acc: 84.114% (42057/50000)\n",
            "99 100 Loss: 0.745 | Acc: 76.390% (7639/10000)\n",
            "\n",
            "Epoch: 26\n",
            "390 391 Loss: 0.458 | Acc: 84.328% (42164/50000)\n",
            "99 100 Loss: 0.531 | Acc: 82.060% (8206/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 27\n",
            "390 391 Loss: 0.448 | Acc: 84.602% (42301/50000)\n",
            "99 100 Loss: 0.657 | Acc: 78.760% (7876/10000)\n",
            "\n",
            "Epoch: 28\n",
            "390 391 Loss: 0.450 | Acc: 84.646% (42323/50000)\n",
            "99 100 Loss: 0.671 | Acc: 77.700% (7770/10000)\n",
            "\n",
            "Epoch: 29\n",
            "390 391 Loss: 0.446 | Acc: 84.648% (42324/50000)\n",
            "99 100 Loss: 0.563 | Acc: 81.780% (8178/10000)\n",
            "\n",
            "Epoch: 30\n",
            "390 391 Loss: 0.443 | Acc: 84.936% (42468/50000)\n",
            "99 100 Loss: 0.653 | Acc: 78.010% (7801/10000)\n",
            "\n",
            "Epoch: 31\n",
            "390 391 Loss: 0.446 | Acc: 84.830% (42415/50000)\n",
            "99 100 Loss: 0.665 | Acc: 78.370% (7837/10000)\n",
            "\n",
            "Epoch: 32\n",
            "390 391 Loss: 0.434 | Acc: 85.128% (42564/50000)\n",
            "99 100 Loss: 0.869 | Acc: 73.670% (7367/10000)\n",
            "\n",
            "Epoch: 33\n",
            "390 391 Loss: 0.434 | Acc: 85.052% (42526/50000)\n",
            "99 100 Loss: 0.578 | Acc: 81.500% (8150/10000)\n",
            "\n",
            "Epoch: 34\n",
            "390 391 Loss: 0.431 | Acc: 85.338% (42669/50000)\n",
            "99 100 Loss: 0.704 | Acc: 78.060% (7806/10000)\n",
            "\n",
            "Epoch: 35\n",
            "390 391 Loss: 0.432 | Acc: 85.234% (42617/50000)\n",
            "99 100 Loss: 0.580 | Acc: 81.560% (8156/10000)\n",
            "\n",
            "Epoch: 36\n",
            "390 391 Loss: 0.421 | Acc: 85.556% (42778/50000)\n",
            "99 100 Loss: 0.541 | Acc: 82.050% (8205/10000)\n",
            "\n",
            "Epoch: 37\n",
            "390 391 Loss: 0.420 | Acc: 85.394% (42697/50000)\n",
            "99 100 Loss: 0.692 | Acc: 77.640% (7764/10000)\n",
            "\n",
            "Epoch: 38\n",
            "390 391 Loss: 0.422 | Acc: 85.568% (42784/50000)\n",
            "99 100 Loss: 0.779 | Acc: 74.790% (7479/10000)\n",
            "\n",
            "Epoch: 39\n",
            "390 391 Loss: 0.417 | Acc: 85.816% (42908/50000)\n",
            "99 100 Loss: 0.605 | Acc: 79.410% (7941/10000)\n",
            "\n",
            "Epoch: 40\n",
            "390 391 Loss: 0.419 | Acc: 85.730% (42865/50000)\n",
            "99 100 Loss: 0.614 | Acc: 79.650% (7965/10000)\n",
            "\n",
            "Epoch: 41\n",
            "390 391 Loss: 0.410 | Acc: 85.984% (42992/50000)\n",
            "99 100 Loss: 0.630 | Acc: 79.690% (7969/10000)\n",
            "\n",
            "Epoch: 42\n",
            "390 391 Loss: 0.410 | Acc: 85.910% (42955/50000)\n",
            "99 100 Loss: 0.732 | Acc: 77.730% (7773/10000)\n",
            "\n",
            "Epoch: 43\n",
            "390 391 Loss: 0.403 | Acc: 86.186% (43093/50000)\n",
            "99 100 Loss: 0.702 | Acc: 77.900% (7790/10000)\n",
            "\n",
            "Epoch: 44\n",
            "390 391 Loss: 0.408 | Acc: 86.108% (43054/50000)\n",
            "99 100 Loss: 0.727 | Acc: 77.580% (7758/10000)\n",
            "\n",
            "Epoch: 45\n",
            "390 391 Loss: 0.401 | Acc: 86.352% (43176/50000)\n",
            "99 100 Loss: 0.625 | Acc: 79.760% (7976/10000)\n",
            "\n",
            "Epoch: 46\n",
            "390 391 Loss: 0.399 | Acc: 86.330% (43165/50000)\n",
            "99 100 Loss: 0.578 | Acc: 80.670% (8067/10000)\n",
            "\n",
            "Epoch: 47\n",
            "390 391 Loss: 0.399 | Acc: 86.336% (43168/50000)\n",
            "99 100 Loss: 0.795 | Acc: 74.040% (7404/10000)\n",
            "\n",
            "Epoch: 48\n",
            "390 391 Loss: 0.401 | Acc: 86.156% (43078/50000)\n",
            "99 100 Loss: 0.772 | Acc: 75.060% (7506/10000)\n",
            "\n",
            "Epoch: 49\n",
            "390 391 Loss: 0.399 | Acc: 86.358% (43179/50000)\n",
            "99 100 Loss: 0.534 | Acc: 81.900% (8190/10000)\n",
            "\n",
            "Epoch: 50\n",
            "390 391 Loss: 0.390 | Acc: 86.622% (43311/50000)\n",
            "99 100 Loss: 0.754 | Acc: 75.120% (7512/10000)\n",
            "\n",
            "Epoch: 51\n",
            "390 391 Loss: 0.388 | Acc: 86.776% (43388/50000)\n",
            "99 100 Loss: 0.595 | Acc: 80.780% (8078/10000)\n",
            "\n",
            "Epoch: 52\n",
            "390 391 Loss: 0.387 | Acc: 86.760% (43380/50000)\n",
            "99 100 Loss: 0.589 | Acc: 80.660% (8066/10000)\n",
            "\n",
            "Epoch: 53\n",
            "390 391 Loss: 0.393 | Acc: 86.380% (43190/50000)\n",
            "99 100 Loss: 0.625 | Acc: 78.690% (7869/10000)\n",
            "\n",
            "Epoch: 54\n",
            "390 391 Loss: 0.385 | Acc: 86.776% (43388/50000)\n",
            "99 100 Loss: 0.659 | Acc: 79.240% (7924/10000)\n",
            "\n",
            "Epoch: 55\n",
            "390 391 Loss: 0.375 | Acc: 87.222% (43611/50000)\n",
            "99 100 Loss: 0.529 | Acc: 82.160% (8216/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 56\n",
            "390 391 Loss: 0.385 | Acc: 86.716% (43358/50000)\n",
            "99 100 Loss: 0.611 | Acc: 80.080% (8008/10000)\n",
            "\n",
            "Epoch: 57\n",
            "390 391 Loss: 0.380 | Acc: 86.890% (43445/50000)\n",
            "99 100 Loss: 0.573 | Acc: 81.160% (8116/10000)\n",
            "\n",
            "Epoch: 58\n",
            "390 391 Loss: 0.371 | Acc: 87.290% (43645/50000)\n",
            "99 100 Loss: 0.643 | Acc: 79.420% (7942/10000)\n",
            "\n",
            "Epoch: 59\n",
            "390 391 Loss: 0.368 | Acc: 87.368% (43684/50000)\n",
            "99 100 Loss: 0.647 | Acc: 79.920% (7992/10000)\n",
            "\n",
            "Epoch: 60\n",
            "390 391 Loss: 0.372 | Acc: 87.254% (43627/50000)\n",
            "99 100 Loss: 0.540 | Acc: 82.760% (8276/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 61\n",
            "390 391 Loss: 0.371 | Acc: 87.310% (43655/50000)\n",
            "99 100 Loss: 0.528 | Acc: 81.980% (8198/10000)\n",
            "\n",
            "Epoch: 62\n",
            "390 391 Loss: 0.360 | Acc: 87.710% (43855/50000)\n",
            "99 100 Loss: 0.519 | Acc: 82.390% (8239/10000)\n",
            "\n",
            "Epoch: 63\n",
            "390 391 Loss: 0.367 | Acc: 87.516% (43758/50000)\n",
            "99 100 Loss: 0.552 | Acc: 81.950% (8195/10000)\n",
            "\n",
            "Epoch: 64\n",
            "390 391 Loss: 0.360 | Acc: 87.668% (43834/50000)\n",
            "99 100 Loss: 0.689 | Acc: 78.370% (7837/10000)\n",
            "\n",
            "Epoch: 65\n",
            "390 391 Loss: 0.355 | Acc: 87.810% (43905/50000)\n",
            "99 100 Loss: 0.512 | Acc: 82.850% (8285/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 66\n",
            "390 391 Loss: 0.357 | Acc: 87.800% (43900/50000)\n",
            "99 100 Loss: 0.554 | Acc: 81.030% (8103/10000)\n",
            "\n",
            "Epoch: 67\n",
            "390 391 Loss: 0.356 | Acc: 87.736% (43868/50000)\n",
            "99 100 Loss: 0.404 | Acc: 86.710% (8671/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 68\n",
            "390 391 Loss: 0.349 | Acc: 87.828% (43914/50000)\n",
            "99 100 Loss: 0.552 | Acc: 81.330% (8133/10000)\n",
            "\n",
            "Epoch: 69\n",
            "390 391 Loss: 0.348 | Acc: 88.200% (44100/50000)\n",
            "99 100 Loss: 0.614 | Acc: 80.210% (8021/10000)\n",
            "\n",
            "Epoch: 70\n",
            "390 391 Loss: 0.343 | Acc: 88.148% (44074/50000)\n",
            "99 100 Loss: 0.578 | Acc: 81.470% (8147/10000)\n",
            "\n",
            "Epoch: 71\n",
            "390 391 Loss: 0.351 | Acc: 88.200% (44100/50000)\n",
            "99 100 Loss: 0.472 | Acc: 84.550% (8455/10000)\n",
            "\n",
            "Epoch: 72\n",
            "390 391 Loss: 0.338 | Acc: 88.522% (44261/50000)\n",
            "99 100 Loss: 0.439 | Acc: 85.490% (8549/10000)\n",
            "\n",
            "Epoch: 73\n",
            "390 391 Loss: 0.338 | Acc: 88.384% (44192/50000)\n",
            "99 100 Loss: 0.531 | Acc: 82.650% (8265/10000)\n",
            "\n",
            "Epoch: 74\n",
            "390 391 Loss: 0.338 | Acc: 88.324% (44162/50000)\n",
            "99 100 Loss: 0.518 | Acc: 83.310% (8331/10000)\n",
            "\n",
            "Epoch: 75\n",
            "390 391 Loss: 0.332 | Acc: 88.576% (44288/50000)\n",
            "99 100 Loss: 0.444 | Acc: 85.040% (8504/10000)\n",
            "\n",
            "Epoch: 76\n",
            "390 391 Loss: 0.336 | Acc: 88.360% (44180/50000)\n",
            "99 100 Loss: 0.595 | Acc: 80.710% (8071/10000)\n",
            "\n",
            "Epoch: 77\n",
            "390 391 Loss: 0.333 | Acc: 88.536% (44268/50000)\n",
            "99 100 Loss: 0.515 | Acc: 83.160% (8316/10000)\n",
            "\n",
            "Epoch: 78\n",
            "390 391 Loss: 0.333 | Acc: 88.622% (44311/50000)\n",
            "99 100 Loss: 0.604 | Acc: 81.030% (8103/10000)\n",
            "\n",
            "Epoch: 79\n",
            "390 391 Loss: 0.323 | Acc: 88.872% (44436/50000)\n",
            "99 100 Loss: 0.516 | Acc: 83.220% (8322/10000)\n",
            "\n",
            "Epoch: 80\n",
            "390 391 Loss: 0.325 | Acc: 88.750% (44375/50000)\n",
            "99 100 Loss: 0.526 | Acc: 82.900% (8290/10000)\n",
            "\n",
            "Epoch: 81\n",
            "390 391 Loss: 0.321 | Acc: 88.824% (44412/50000)\n",
            "99 100 Loss: 0.478 | Acc: 84.570% (8457/10000)\n",
            "\n",
            "Epoch: 82\n",
            "390 391 Loss: 0.320 | Acc: 89.054% (44527/50000)\n",
            "99 100 Loss: 0.475 | Acc: 84.470% (8447/10000)\n",
            "\n",
            "Epoch: 83\n",
            "390 391 Loss: 0.316 | Acc: 89.214% (44607/50000)\n",
            "99 100 Loss: 0.498 | Acc: 83.380% (8338/10000)\n",
            "\n",
            "Epoch: 84\n",
            "390 391 Loss: 0.320 | Acc: 88.946% (44473/50000)\n",
            "99 100 Loss: 0.449 | Acc: 85.250% (8525/10000)\n",
            "\n",
            "Epoch: 85\n",
            "390 391 Loss: 0.314 | Acc: 89.110% (44555/50000)\n",
            "99 100 Loss: 0.500 | Acc: 83.340% (8334/10000)\n",
            "\n",
            "Epoch: 86\n",
            "390 391 Loss: 0.310 | Acc: 89.310% (44655/50000)\n",
            "99 100 Loss: 0.482 | Acc: 85.120% (8512/10000)\n",
            "\n",
            "Epoch: 87\n",
            "390 391 Loss: 0.304 | Acc: 89.604% (44802/50000)\n",
            "99 100 Loss: 0.423 | Acc: 85.710% (8571/10000)\n",
            "\n",
            "Epoch: 88\n",
            "390 391 Loss: 0.305 | Acc: 89.542% (44771/50000)\n",
            "99 100 Loss: 0.543 | Acc: 82.770% (8277/10000)\n",
            "\n",
            "Epoch: 89\n",
            "390 391 Loss: 0.304 | Acc: 89.514% (44757/50000)\n",
            "99 100 Loss: 0.496 | Acc: 84.310% (8431/10000)\n",
            "\n",
            "Epoch: 90\n",
            "390 391 Loss: 0.299 | Acc: 89.616% (44808/50000)\n",
            "99 100 Loss: 0.460 | Acc: 84.850% (8485/10000)\n",
            "\n",
            "Epoch: 91\n",
            "390 391 Loss: 0.299 | Acc: 89.770% (44885/50000)\n",
            "99 100 Loss: 0.510 | Acc: 83.510% (8351/10000)\n",
            "\n",
            "Epoch: 92\n",
            "390 391 Loss: 0.296 | Acc: 89.850% (44925/50000)\n",
            "99 100 Loss: 0.541 | Acc: 83.000% (8300/10000)\n",
            "\n",
            "Epoch: 93\n",
            "390 391 Loss: 0.295 | Acc: 89.944% (44972/50000)\n",
            "99 100 Loss: 0.652 | Acc: 80.200% (8020/10000)\n",
            "\n",
            "Epoch: 94\n",
            "390 391 Loss: 0.288 | Acc: 90.090% (45045/50000)\n",
            "99 100 Loss: 0.608 | Acc: 80.480% (8048/10000)\n",
            "\n",
            "Epoch: 95\n",
            "390 391 Loss: 0.291 | Acc: 90.184% (45092/50000)\n",
            "99 100 Loss: 0.417 | Acc: 86.270% (8627/10000)\n",
            "\n",
            "Epoch: 96\n",
            "390 391 Loss: 0.288 | Acc: 90.022% (45011/50000)\n",
            "99 100 Loss: 0.423 | Acc: 86.460% (8646/10000)\n",
            "\n",
            "Epoch: 97\n",
            "390 391 Loss: 0.287 | Acc: 90.174% (45087/50000)\n",
            "99 100 Loss: 0.504 | Acc: 83.780% (8378/10000)\n",
            "\n",
            "Epoch: 98\n",
            "390 391 Loss: 0.278 | Acc: 90.402% (45201/50000)\n",
            "99 100 Loss: 0.462 | Acc: 84.800% (8480/10000)\n",
            "\n",
            "Epoch: 99\n",
            "390 391 Loss: 0.281 | Acc: 90.242% (45121/50000)\n",
            "99 100 Loss: 0.563 | Acc: 82.260% (8226/10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WEriQyYJ6oVa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}